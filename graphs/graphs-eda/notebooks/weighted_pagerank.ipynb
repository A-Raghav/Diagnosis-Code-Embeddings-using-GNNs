{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from sklearn.manifold import TSNE\n",
    "from time import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652ab38",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "metadata_columns = ['Diagnosis Code','Description','CMS-HCC Model Category V24']\n",
    "inpatient_data_columns = ['empi','visit_id','visit_start_date','primary_diagnosis']\n",
    "outpatient_data_columns = ['empi','visit_id','last_date_of_service','primary_diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5562467",
   "metadata": {},
   "source": [
    "#### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d14ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the risk adjustment metadata file\n",
    "metadata = pd.read_csv(\"../data/metadata/2022 Midyear_Final ICD-10-CM Mappings.csv\")\n",
    "\n",
    "# reading the inpatient and outpatient data \n",
    "inpatient_data = pd.read_csv(\"../data/patient-data/df_preprocessed.csv\")\n",
    "outpatient_data = pd.read_csv(\"../data/patient-data/df_outpatient.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906c10c6",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_metadata(metadata):\n",
    "    \"\"\"\n",
    "    Cleans and prepares the HCC metadata file\n",
    "    \"\"\"\n",
    "    # cleaning the risk adjustment metadata file\n",
    "    metadata = metadata.iloc[2:-7,:]\n",
    "    metadata.replace(r'\\n',' ', regex=True, inplace=True)\n",
    "    metadata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # creating the cleaned risk adjustment metadata dataframe\n",
    "    new_metadata = pd.DataFrame(metadata.iloc[1:,:])\n",
    "    new_metadata.columns = metadata.iloc[0,:].tolist()\n",
    "\n",
    "    # filtering only the required columns from patient data and metadata\n",
    "    new_metadata = new_metadata.loc[:,metadata_columns]\n",
    "    new_metadata.columns = ['pd','dscr','hcc']\n",
    "    new_metadata.loc[:,\"hcc\"] = new_metadata.hcc.fillna(0).astype('int')\n",
    "\n",
    "    return new_metadata\n",
    "\n",
    "\n",
    "def prepare_patient_data(inpatient_data, outpatient_data):\n",
    "    \"\"\"\n",
    "    filters only the required columns from inpatient and outpatient data\n",
    "    \"\"\"\n",
    "    inpatient_data = inpatient_data.loc[:,inpatient_data_columns]\n",
    "    outpatient_data = outpatient_data.loc[:,outpatient_data_columns]\n",
    "\n",
    "    inpatient_data.columns = ['empi','vid','vdt','pd']\n",
    "    outpatient_data.columns = ['empi','vid','vdt','pd']\n",
    "\n",
    "    return inpatient_data, outpatient_data\n",
    "\n",
    "\n",
    "def create_patient_hcc_mapping(patient_df, hcc_df):\n",
    "    \"\"\"Maps ICD-10 codes to HCCs and prepares the data for \n",
    "    processing into adjacency matrices\n",
    "\n",
    "    Args:\n",
    "        patient_df (_type_): IP hospitalisation data\n",
    "        hcc_df (_type_): HCC mapping for various (sub)models\n",
    "\n",
    "    Returns:\n",
    "        data: preprocessed data\n",
    "    \"\"\"\n",
    "    join_params = {\n",
    "    'left':patient_df,\n",
    "    'right':metadata,\n",
    "    'on':'pd',\n",
    "    'how':'left'\n",
    "    }\n",
    "    data_merged = pd.merge(**join_params)\n",
    "    data_hcc_dummies = pd.get_dummies(data_merged.hcc).iloc[:,1:]\n",
    "    data = pd.concat([data_merged,data_hcc_dummies], axis=1)\n",
    "    agg_dict = dict(zip(data_hcc_dummies.columns,np.repeat('sum', len(data_hcc_dummies.columns))))\n",
    "    data = data.groupby('empi', as_index=False).aggregate(agg_dict)\n",
    "    print(\"HCC Mapping done...\")\n",
    "    data.iloc[:,1:] = data.iloc[:,1:].ne(0)*1\n",
    "    return data\n",
    "\n",
    "\n",
    "def filter_edge_data_by_node(node, edge_data=edge_data):\n",
    "    n_i = edge_data.columns[0]\n",
    "    n_j = edge_data.columns[1]\n",
    "    weight = edge_data.columns[2]\n",
    "    filter1 = edge_data[n_i]==node\n",
    "    filter2 = edge_data[n_j]==node\n",
    "    return edge_data[filter1 | filter2].sort_values(weight)\n",
    "\n",
    "\n",
    "def filter_edge_data_by_source_node(node, edge_data=edge_data):\n",
    "    n_i = edge_data.columns[0]\n",
    "    weight = edge_data.columns[2]\n",
    "    filter1 = edge_data[n_i]==node\n",
    "    return edge_data[filter1].sort_values(weight)\n",
    "\n",
    "\n",
    "def filter_edge_data_by_target_node(node, edge_data=edge_data):\n",
    "    n_j = edge_data.columns[1]\n",
    "    weight = edge_data.columns[2]\n",
    "    filter1 = edge_data[n_j]==node\n",
    "    return edge_data[filter1].sort_values(weight)\n",
    "\n",
    "\n",
    "def plot_disease_graph(node_data, edge_data):\n",
    "\n",
    "    # Storing data in a networkx graph object (for graph visualisation)\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for node in node_data.index:\n",
    "        G.add_node(node, hcc_count=node_data[node])\n",
    "\n",
    "    for _, edges in edge_data.iterrows():\n",
    "        G.add_edge(edges[0],edges[1], e_ij_sim=edges[4])\n",
    "\n",
    "    # plotting IP disease graph\n",
    "    plt.figure(figsize=(50,40))\n",
    "    pos = nx.spring_layout(G, weight='e_ij_sim')\n",
    "    nx.draw(G, pos)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=40)\n",
    "    nx.draw_networkx_edge_labels(G, pos)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_adjacency_matrices(edge_data):\n",
    "    \"\"\"\n",
    "    Creates adjacency matrix of order 1 and proximity matrix\n",
    "    of order 2\"\"\"\n",
    "    hcc_set = set(edge_data.n_i) | set(edge_data.n_j)\n",
    "    adjacency_matrix_1 = pd.DataFrame(0,columns=hcc_set, index=hcc_set)\n",
    "    adjacency_matrix_2 = pd.DataFrame(0,columns=hcc_set, index=hcc_set)\n",
    "\n",
    "    for _, edge in edge_data.iterrows():\n",
    "        adjacency_matrix_1.loc[edge[0],edge[1]] = 1\n",
    "        adjacency_matrix_1.loc[edge[1],edge[0]] = 1\n",
    "    \n",
    "    for i in range(adjacency_matrix_1.shape[0]):\n",
    "        for j in range(i+1, adjacency_matrix_1.shape[0]):\n",
    "            adjacency_matrix_2.iloc[i,j] = np.sum(adjacency_matrix_1.iloc[i,:]*adjacency_matrix_1.iloc[j,:])\n",
    "            adjacency_matrix_2.iloc[j,i] = np.sum(adjacency_matrix_1.iloc[i,:]*adjacency_matrix_1.iloc[j,:])\n",
    "    \n",
    "    return adjacency_matrix_1, adjacency_matrix_2\n",
    "\n",
    "\n",
    "def create_stellargraph(node_data, edge_data):\n",
    "    node_data_df = pd.DataFrame({'x':node_data.values}, index=node_data.index).astype(float)\n",
    "    edge_data_df = edge_data[['n_i','n_j','e_ij_sim']].copy().astype(float)\n",
    "    edge_data_df.columns = ['source','target','weight']\n",
    "\n",
    "    G = StellarGraph(node_data_df, edges=edge_data_df)\n",
    "    return G\n",
    "\n",
    "\n",
    "def biased_random_walk(G, weighted=False, n=100):\n",
    "    \"\"\"\n",
    "    Performs biased random walk, return walks paths\"\"\"\n",
    "    rw = BiasedRandomWalk(G)\n",
    "\n",
    "    walks = rw.run(\n",
    "        nodes=list(G.nodes()),  # root nodes\n",
    "        length=20,  # maximum length of a random walk\n",
    "        n=n,  # number of random walks per root node\n",
    "        p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "        q=2,  # Defines (unormalised) probability, 1/q, for moving away from source node (intuitively, ratio of BFS:DFS)\n",
    "        weighted=weighted,\n",
    "        seed=42\n",
    "    )\n",
    "    print(\"Number of random walks: {}\".format(len(walks)))\n",
    "    return walks\n",
    "\n",
    "\n",
    "def create_node_embeddings(walks, nodes_list):\n",
    "    \n",
    "    model = Word2Vec(walks, window=5, min_count=0, sg=1, workers=1)\n",
    "    \n",
    "    embeddings = pd.DataFrame()\n",
    "    for node in nodes_list:\n",
    "        embeddings[str(node)] = model.wv[int(node)]\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def TSNE_plot_node_embeddings(embeddings):\n",
    "\n",
    "    transform = TSNE  # PCA\n",
    "    trans = transform(n_components=2)\n",
    "    tsne_coordinates = trans.fit_transform(embeddings.T)\n",
    "\n",
    "    alpha = 0.7\n",
    "\n",
    "    plt.figure(figsize=(20, 18))\n",
    "    plt.axes().set(aspect=\"equal\")\n",
    "    plt.scatter(\n",
    "        tsne_coordinates[:, 0],\n",
    "        tsne_coordinates[:, 1],\n",
    "        cmap=\"jet\",\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    plt.title(\"{} visualization of node embeddings\".format(transform.__name__))\n",
    "    for i in range(tsne_coordinates.shape[0]):\n",
    "        plt.annotate(embeddings.columns[i], (tsne_coordinates[i,0], tsne_coordinates[i,1]))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_embeddings_correlation(embeddings):\n",
    "    plt.figure(figsize=(20,16))\n",
    "    mask = np.triu(np.ones_like(embeddings.corr(), dtype=bool))\n",
    "    sns.heatmap(embeddings.corr(), mask=mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36863f3",
   "metadata": {},
   "source": [
    "#### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing cleaned risk adjustment metadata\n",
    "metadata = prepare_metadata(metadata)\n",
    "\n",
    "# filtering only the required columns from inpatient and outpatient data\n",
    "inpatient_data, outpatient_data = prepare_patient_data(inpatient_data, outpatient_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66e3a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient Data: (20541, 4)\n",
      "          empi                               vid         vdt     pd\n",
      "0  M0000040556  nM0000040556:1088927097671487508  2018-01-20  K5641\n",
      "1  M0000040556  nM0000040556:1801256381439324181  2018-02-04   G458\n",
      "2  M0000040556  nM0000040556:1339948222969081413  2018-02-18   K254\n",
      "3  M0000040556  nM0000040556:1014145580172622435  2018-04-13  R5381\n",
      "4  M0000040556   nM0000040556:130095445752129940  2018-06-27   I160\n",
      "\n",
      "Outpatient Data: (371942, 4)\n",
      "          empi                               vid         vdt      pd\n",
      "0  M0000040556  nM0000040556:1002776852509021534  2018-06-03    N183\n",
      "1  M0000040556  nM0000040556:1003353323421991148  2018-11-07  I87312\n",
      "2  M0000040556  nM0000040556:1005893621657975338  2019-05-06   I2510\n",
      "3  M0000040556  nM0000040556:1006150904029681462  2018-10-26  I69354\n",
      "4  M0000040556  nM0000040556:1007502285099269954  2018-12-03   K8590\n",
      "\n",
      "Metadata: (10981, 3)\n",
      "      pd                   dscr  hcc\n",
      "1  A0103      Typhoid pneumonia  115\n",
      "2  A0104      Typhoid arthritis   39\n",
      "3  A0105  Typhoid osteomyelitis   39\n",
      "4   A021      Salmonella sepsis    2\n",
      "5  A0222   Salmonella pneumonia  115\n"
     ]
    }
   ],
   "source": [
    "print(f\"Patient Data: {inpatient_data.shape}\\n{inpatient_data.head()}\")\n",
    "print(f\"\\nOutpatient Data: {outpatient_data.shape}\\n{outpatient_data.head()}\")\n",
    "print(f\"\\nMetadata: {metadata.shape}\\n{metadata.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining IP & OP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = pd.concat([inpatient_data, outpatient_data], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_params = {\n",
    "'left':data_combined,\n",
    "'right':metadata,\n",
    "'on':'pd',\n",
    "'how':'left'\n",
    "}\n",
    "\n",
    "data_combined_pagerank = pd.merge(**join_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined_pagerank['hcc'] = data_combined_pagerank.hcc.replace(0,np.nan)\n",
    "data_combined_pagerank = data_combined_pagerank.dropna()\n",
    "data_combined_pagerank = data_combined_pagerank.sort_values(['empi','vdt']).reset_index(drop=True)\n",
    "data_combined_pagerank[\"hcc_nxt\"] = data_combined_pagerank.hcc.shift(-1)\n",
    "index_drop = data_combined_pagerank.groupby('empi').tail(1).index\n",
    "data_combined_pagerank = data_combined_pagerank.drop(index_drop)\n",
    "data_combined_pagerank = data_combined_pagerank[data_combined_pagerank.hcc!=data_combined_pagerank.hcc_nxt].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined_pagerank['sno'] = (data_combined_pagerank.empi+\"-src-hcc\"+data_combined_pagerank.hcc.astype(str)+\"-tgt-hcc\"+data_combined_pagerank.hcc_nxt.astype(str))\n",
    "data_combined_pagerank = data_combined_pagerank.groupby('sno').head(1).reset_index(drop=True).drop(['sno'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined_pagerank = data_combined_pagerank[['hcc','hcc_nxt']].astype(str)\n",
    "data_combined_pagerank['edge'] = data_combined_pagerank.hcc + \", \" + data_combined_pagerank.hcc_nxt\n",
    "data_combined_pagerank['cnt'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_data = data_combined_pagerank.groupby(['hcc','hcc_nxt'], as_index=False).agg({'cnt':'count'}).astype(float)\n",
    "edge_data.columns = ['source','target','weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2752, 3)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(561, 3)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_data = edge_data[edge_data.weight>=10] # remove edges having less than 10 count (weight)\n",
    "edge_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_edgelist(edge_data, 'source', 'target', create_using=nx.DiGraph)\n",
    "G_weighted = nx.from_pandas_edgelist(edge_data, 'source', 'target', create_using=nx.DiGraph, edge_attr='weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_pagerank = nx.pagerank(G_weighted, alpha=0.85)\n",
    "pagerank_importances = pd.Series(weighted_pagerank.values(), index=weighted_pagerank.keys()).sort_values()\n",
    "pagerank_importances.index.name='hcc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relational Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_in_degree(node, edge_data=edge_data):\n",
    "    \"\"\"Computes the in-degree of node\n",
    "\n",
    "    Args:\n",
    "        node (_type_): _description_\n",
    "        edge_data (_type_): _description_\n",
    "    \"\"\"\n",
    "    return filter_edge_data_by_target_node(node, edge_data).weight.sum()\n",
    "\n",
    "\n",
    "def identify_outgoing_neighbours(base_nodes, edge_data=edge_data):\n",
    "    \"\"\"returns the set of outgoing neighbours of base nodes\n",
    "    Args:\n",
    "        base_nodes (list): list of base disease\n",
    "        edge_data (_type_, optional): _description_. Defaults to edge_data.\n",
    "\n",
    "    Returns:\n",
    "        _type_: set of neighbour disease nodes\n",
    "    \"\"\"\n",
    "    neighbours = set()\n",
    "    for node in base_nodes:\n",
    "        neighbours = neighbours | set(filter_edge_data_by_source_node(node).target)\n",
    "    return neighbours\n",
    "\n",
    "\n",
    "def compute_relational_score(base_nodes, edge_data=edge_data):\n",
    "    score_dict = {}\n",
    "    tgt_nodes = identify_outgoing_neighbours(base_nodes)\n",
    "    for node in tgt_nodes:\n",
    "        edge_data_by_tgt_node = filter_edge_data_by_target_node(node)\n",
    "        \n",
    "        weights = edge_data_by_tgt_node[edge_data_by_tgt_node.source.isin(base_nodes)].rename(index=edge_data.source).weight.sort_index()\n",
    "        importances = pagerank_importances[pagerank_importances.index.isin(base_nodes)].sort_index()\n",
    "        in_degree = compute_in_degree(node)\n",
    "\n",
    "        score = (weights*importances).sum()\n",
    "        score_dict[node] = score\n",
    "    return pd.Series(score_dict).sort_values().replace(0,np.nan).dropna()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.0     0.150335\n",
       "111.0    0.150335\n",
       "136.0    0.180402\n",
       "2.0      0.195435\n",
       "19.0     0.195435\n",
       "99.0     0.210469\n",
       "18.0     0.210469\n",
       "85.0     0.285636\n",
       "96.0     0.345770\n",
       "100.0    0.481071\n",
       "dtype: float64"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_nodes = [103]\n",
    "scores = compute_relational_score(base_nodes)\n",
    "scores.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.15 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
